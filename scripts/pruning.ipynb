{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dac\n",
    "from dac.model.dac import EncoderBlock, DecoderBlock, ResidualUnit\n",
    "from dac.nn.layers import Snake1d\n",
    "from audiotools import AudioSignal\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = dac.utils.download(model_type=\"44khz\")\n",
    "model = dac.DAC.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAC(\n",
       "  (encoder): Encoder(\n",
       "    (block): Sequential(\n",
       "      (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "        )\n",
       "      )\n",
       "      (5): Snake1d()\n",
       "      (6): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (quantizer): ResidualVectorQuantize(\n",
       "    (quantizers): ModuleList(\n",
       "      (0-8): 9 x VectorQuantize(\n",
       "        (in_proj): Conv1d(1024, 8, kernel_size=(1,), stride=(1,))\n",
       "        (out_proj): Conv1d(8, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (codebook): Embedding(1024, 8)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (model): Sequential(\n",
       "      (0): Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Snake1d()\n",
       "      (6): Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "1 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "2 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
      "  )\n",
      ")\n",
      "3 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "  )\n",
      ")\n",
      "4 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "  )\n",
      ")\n",
      "5 Snake1d()\n",
      "6 Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n"
     ]
    }
   ],
   "source": [
    "conv_layers = []\n",
    "\n",
    "for idx, layer in enumerate(model.encoder.block):\n",
    "    print(idx, layer)\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        # print(\"nn.Conv1d here\")\n",
    "        conv_layers.append(layer)\n",
    "    elif isinstance(layer, EncoderBlock):\n",
    "        encoder_block_layers = layer.block\n",
    "        # print(\"encoder block here\")\n",
    "        for encoder_block_layer in encoder_block_layers:\n",
    "            if isinstance(encoder_block_layer, nn.Conv1d):\n",
    "                # print(\"nn.Conv1d here\")\n",
    "                conv_layers.append(encoder_block_layer)\n",
    "            elif isinstance(encoder_block_layer, ResidualUnit):\n",
    "                residual_unit_layers = encoder_block_layer.block\n",
    "                # print(\"residual unit here\")\n",
    "                for residual_unit_layer in residual_unit_layers:\n",
    "                    if isinstance(residual_unit_layer, nn.Conv1d):\n",
    "                        # print(\"nn.Conv1d here\")\n",
    "                        conv_layers.append(residual_unit_layer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "1 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "3 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "4 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "5 Snake1d()\n",
      "6 Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "7 Tanh()\n"
     ]
    }
   ],
   "source": [
    "conv_layers = []\n",
    "\n",
    "for idx, layer in enumerate(model.decoder.model):\n",
    "    print(idx, layer)\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        # print(\"nn.Conv1d here\")\n",
    "        conv_layers.append(layer)\n",
    "    elif isinstance(layer, DecoderBlock):\n",
    "        decoder_block_layers = layer.block\n",
    "        # print(\"encoder block here\")\n",
    "        for decoder_block_layer in decoder_block_layers:\n",
    "            if isinstance(decoder_block_layer, nn.Conv1d):\n",
    "                # print(\"nn.Conv1d here\")\n",
    "                conv_layers.append(decoder_block_layer)\n",
    "            elif isinstance(decoder_block_layer, ResidualUnit):\n",
    "                residual_unit_layers = decoder_block_layer.block\n",
    "                # print(\"residual unit here\")\n",
    "                for residual_unit_layer in residual_unit_layers:\n",
    "                    if isinstance(residual_unit_layer, nn.Conv1d):\n",
    "                        # print(\"nn.Conv1d here\")\n",
    "                        conv_layers.append(residual_unit_layer)\n",
    "\n",
    "# conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_wn(layers):\n",
    "    for idx, layer in enumerate(layers):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            wn_removed_layer = nn.utils.remove_weight_norm(layer)\n",
    "            layers[idx] = wn_removed_layer\n",
    "        elif hasattr(layer, 'block'):\n",
    "            remove_wn(layer.block)\n",
    "\n",
    "def find_conv_layers(layers, conv_layers):\n",
    "    for idx, layer in enumerate(layers):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            conv_layers.append(layer)\n",
    "        elif hasattr(layer, 'block'):\n",
    "            find_conv_layers(layer.block, conv_layers)\n",
    "\n",
    "def find_layers(layers, conv_layers, snake_layers):\n",
    "    for idx, layer in enumerate(layers):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            # print(idx, layer, layer.weight.shape, layer.bias.shape)\n",
    "            conv_layers.append(layer)\n",
    "            if len(conv_layers) != len(snake_layers):\n",
    "                snake_layers.append(None)\n",
    "        elif isinstance(layer, Snake1d):\n",
    "            snake_layers.append(layer)\n",
    "        elif hasattr(layer, 'block'):\n",
    "            find_layers(layer.block, conv_layers, snake_layers)\n",
    "\n",
    "remove_wn(model.encoder.block)\n",
    "remove_wn(model.decoder.model)\n",
    "\n",
    "encoder_conv_layers = []\n",
    "encoder_snake_layers = []\n",
    "\n",
    "decoder_conv_layers = []\n",
    "decoder_snake_layers = []\n",
    "\n",
    "find_layers(model.encoder.block, encoder_conv_layers, encoder_snake_layers)\n",
    "find_layers(model.decoder.model, decoder_conv_layers, decoder_snake_layers)\n",
    "\n",
    "# encoder_conv_layers, decoder_conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort the channels from important to non-important\n",
    "def get_input_channel_importance_conv1d(weight):\n",
    "    in_channels = weight.shape[1]\n",
    "    importances = []\n",
    "    # compute the importance for each input channel\n",
    "    for i_c in range(in_channels):\n",
    "        channel_weight = weight.detach()[:, i_c]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "# function to sort the channels from important to non-important\n",
    "def get_input_channel_importance_convtranspose1d(weight):\n",
    "    in_channels = weight.shape[0]\n",
    "    importances = []\n",
    "    # compute the importance for each input channel\n",
    "    for i_c in range(in_channels):\n",
    "        channel_weight = weight.detach()[i_c, :]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_channel_sorting(model):\n",
    "    # model = copy.deepcopy(model)  # do not modify the original model\n",
    "    # fetch all the conv from the model\n",
    "    all_convs = []\n",
    "    all_snakes = []\n",
    "    find_layers(model.encoder.block, all_convs, all_snakes)\n",
    "    find_layers(model.decoder.model, all_convs, all_snakes)\n",
    "    # iterate through conv layers\n",
    "    for i_conv in range(len(all_convs) - 1):\n",
    "        # each channel sorting index, we need to apply it to:\n",
    "        # - the output dimension of the previous conv\n",
    "        # - the input dimension of the next conv (we compute importance here)\n",
    "        prev_conv = all_convs[i_conv]\n",
    "        next_conv = all_convs[i_conv + 1]\n",
    "        next_snake = all_snakes[i_conv + 1]\n",
    "        # note that we always compute the importance according to input channels\n",
    "        if (isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            importance = get_input_channel_importance_convtranspose1d(next_conv.weight)\n",
    "        else:\n",
    "            importance = get_input_channel_importance_conv1d(next_conv.weight)\n",
    "        # sorting from large to small\n",
    "        sort_idx = torch.argsort(importance, descending=True)\n",
    "        \n",
    "        # apply to the next conv input\n",
    "        if (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n",
    "        elif (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 0, sort_idx))\n",
    "        elif (isinstance(prev_conv, nn.ConvTranspose1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 1, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n",
    "\n",
    "        # prev_conv.bias.copy_(torch.index_select(prev_conv.bias.detach(), 0, sort_idx))\n",
    "        \n",
    "        # if next_snake:\n",
    "        #     next_snake.alpha.data = torch.index_select(next_snake.alpha.data, 1, sort_idx.flip(0))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "def get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:\n",
    "    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n",
    "    Note that preserve_rate = 1. - prune_ratio\n",
    "    \"\"\"\n",
    "    return round((1 - prune_ratio) * channels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def channel_prune(model: nn.Module,\n",
    "                  prune_ratio: Union[List, float]) -> nn.Module:\n",
    "    \"\"\"Apply channel pruning to each of the conv layer in the backbone\n",
    "    Note that for prune_ratio, we can either provide a floating-point number,\n",
    "    indicating that we use a uniform pruning rate for all layers, or a list of\n",
    "    numbers to indicate per-layer pruning rate.\n",
    "    \"\"\"\n",
    "    # sanity check of provided prune_ratio\n",
    "    assert isinstance(prune_ratio, (float, list))\n",
    "    # fetch all the conv from the model\n",
    "    all_convs = []\n",
    "    find_conv_layers(model.encoder.block, all_convs)\n",
    "    find_conv_layers(model.decoder.model, all_convs)\n",
    "    n_conv = len(all_convs)\n",
    "    # note that for the ratios, it affects the previous conv output and next\n",
    "    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n",
    "    if isinstance(prune_ratio, list):\n",
    "        assert len(prune_ratio) == n_conv - 1\n",
    "    else:  # convert float to list\n",
    "        prune_ratio = [prune_ratio] * (n_conv - 1)\n",
    "\n",
    "    # we prune the convs in the backbone with a uniform ratio\n",
    "    # model = copy.deepcopy(model)  # prevent overwrite\n",
    "    # apply pruning. we naively keep the first k channels\n",
    "    for i_ratio, p_ratio in enumerate(prune_ratio):\n",
    "        prev_conv = all_convs[i_ratio]\n",
    "        next_conv = all_convs[i_ratio + 1]\n",
    "        original_channels = prev_conv.out_channels  # same as next_conv.in_channels\n",
    "        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n",
    "\n",
    "        if (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep, :])\n",
    "        elif (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:n_keep])\n",
    "        elif (isinstance(prev_conv, nn.ConvTranspose1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:, :n_keep, :])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep, :])\n",
    "\n",
    "        prev_conv.bias.set_(prev_conv.bias.detach()[:n_keep])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort idx tensor([2, 0, 1])\n",
      "\n",
      "The outputs are the same after sorting.\n",
      "tensor([2, 0, 1]) tensor([0, 1, 2])\n",
      "\n",
      "The outputs are the same after sorting.\n",
      "tensor([2, 0, 1]) tensor([1, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "in_channels = 3\n",
    "oc = 3\n",
    "\n",
    "# Mini test code with Conv1d -> Snake1d -> Conv1d\n",
    "input_tensor = torch.randn(1, 5, 5)\n",
    "\n",
    "conv1d_1 = nn.Conv1d(in_channels=5, out_channels=oc, kernel_size=3, padding=1)\n",
    "\n",
    "snake1d = Snake1d(channels=oc)\n",
    "snake1d.alpha.data = torch.rand(1, oc, 1)  # Random alpha values for each channel\n",
    "\n",
    "conv1d_2 = nn.Conv1d(in_channels=oc, out_channels=1, kernel_size=3, padding=1)\n",
    "\n",
    "original_output = conv1d_1(input_tensor)\n",
    "original_output = snake1d(original_output)\n",
    "original_output = conv1d_2(original_output)\n",
    "\n",
    "importance = get_input_channel_importance_conv1d(conv1d_2.weight)\n",
    "sort_idx = torch.argsort(importance, descending=True)\n",
    "print(\"sort idx\", sort_idx)\n",
    "\n",
    "permutations = list(itertools.permutations(list(range(len(importance)))))\n",
    "\n",
    "for perm in permutations:\n",
    "    perm = torch.tensor(perm)\n",
    "    # print(\"premutation\", perm)\n",
    "\n",
    "    conv1d_1.weight = nn.Parameter(torch.index_select(conv1d_1.weight.detach(), 0, perm))\n",
    "    conv1d_2.weight = nn.Parameter(torch.index_select(conv1d_2.weight.detach(), 1, perm))\n",
    "    snake1d.alpha = nn.Parameter(torch.index_select(snake1d.alpha.data, 1, perm))\n",
    "\n",
    "    sorted_output = conv1d_1(input_tensor)\n",
    "    sorted_output = snake1d(sorted_output)\n",
    "    sorted_output = conv1d_2(sorted_output)\n",
    "\n",
    "    # print(\"Original Output:\")\n",
    "    # print(original_output)\n",
    "    # print(\"\\nSorted Output:\")\n",
    "    # print(sorted_output)\n",
    "\n",
    "    if torch.allclose(original_output, sorted_output, atol=1e-6):\n",
    "        print(\"\\nThe outputs are the same after sorting.\")\n",
    "        print(sort_idx, perm)\n",
    "    # else:\n",
    "    #     print(\"\\nThe outputs are different after sorting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning\n",
      "layer: Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([64, 1, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([128, 64, 4]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([256, 128, 8]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([512, 256, 16]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1024, 512, 16]) \tbias shape: torch.Size([1024])\n",
      "layer: Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) \tweight shape: torch.Size([1024, 1024, 3]) \tbias shape: torch.Size([1024])\n",
      "layer: Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1536, 1024, 7]) \tbias shape: torch.Size([1536])\n",
      "layer: ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1536, 768, 16]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([768, 384, 16]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([384, 192, 8]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([192, 96, 4]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1, 96, 7]) \tbias shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = dac.DAC.load(model_path)\n",
    "\n",
    "print(\"Before pruning\")\n",
    "all_convs = []\n",
    "find_conv_layers(model.encoder.block, all_convs)\n",
    "find_conv_layers(model.decoder.model, all_convs)\n",
    "for conv in all_convs:\n",
    "    print(\"layer:\", conv, \"\\tweight shape:\", conv.weight.shape, \"\\tbias shape:\", conv.bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_pruning_ratio = 0.3\n",
    "sorted_model = apply_channel_sorting(model)\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning\n",
      "layer: Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([45, 1, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([90, 45, 4]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([179, 90, 8]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([358, 179, 16]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([717, 358, 16]) \tbias shape: torch.Size([717])\n",
      "layer: Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) \tweight shape: torch.Size([717, 717, 3]) \tbias shape: torch.Size([717])\n",
      "layer: Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1075, 717, 7]) \tbias shape: torch.Size([1075])\n",
      "layer: ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1075, 538, 16]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([538, 269, 16]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([269, 134, 8]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([134, 67, 4]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1, 67, 7]) \tbias shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning\")\n",
    "all_convs = []\n",
    "find_conv_layers(pruned_model.encoder.block, all_convs)\n",
    "find_conv_layers(pruned_model.decoder.model, all_convs)\n",
    "for conv in all_convs:\n",
    "    print(\"layer:\", conv, \"\\tweight shape:\", conv.weight.shape, \"\\tbias shape:\", conv.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and Decompress + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../samples/UrbanSound8K 7383-3-0-0.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m signal \u001b[38;5;241m=\u001b[39m AudioSignal(audio_file_path)\n\u001b[1;32m---> 17\u001b[0m artifact \u001b[38;5;241m=\u001b[39m \u001b[43mref_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m recons \u001b[38;5;241m=\u001b[39m ref_generator\u001b[38;5;241m.\u001b[39mdecompress(artifact, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m waveform_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\base.py:212\u001b[0m, in \u001b[0;36mCodecMixin.compress\u001b[1;34m(self, audio_path_or_signal, win_duration, verbose, normalize_db, n_quantizers)\u001b[0m\n\u001b[0;32m    210\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39maudio_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    211\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(audio_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m--> 212\u001b[0m _, c, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m codes\u001b[38;5;241m.\u001b[39mappend(c\u001b[38;5;241m.\u001b[39mto(original_device))\n\u001b[0;32m    214\u001b[0m chunk_length \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:243\u001b[0m, in \u001b[0;36mDAC.encode\u001b[1;34m(self, audio_data, n_quantizers)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     audio_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    212\u001b[0m     n_quantizers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m ):\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode given audio data and return quantized latent codes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m            Number of samples in input audio\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     z, codes, latents, commitment_loss, codebook_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(\n\u001b[0;32m    245\u001b[0m         z, n_quantizers\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, codes, latents, commitment_loss, codebook_loss\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:91\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:61\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:36\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     pad \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dac.utils import load_model\n",
    "from dac import DACFile\n",
    "from train import losses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ref_generator = load_model(\n",
    "    model_type=\"44khz\",\n",
    "    model_bitrate=\"8kbps\",\n",
    "    tag=\"latest\",\n",
    "    load_path=model_path,\n",
    ")\n",
    "\n",
    "audio_file_path = \"../samples/UrbanSound8K 7383-3-0-0.wav\"\n",
    "\n",
    "signal = AudioSignal(audio_file_path)\n",
    "\n",
    "artifact = ref_generator.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = ref_generator.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0102), tensor(2.0919), tensor(0.0179))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_wn(ref_generator.encoder.block)\n",
    "remove_wn(ref_generator.decoder.model)\n",
    "\n",
    "artifact = ref_generator.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = ref_generator.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.4130), tensor(11.2193), tensor(0.1503))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_model = apply_channel_sorting(ref_generator)\n",
    "artifact = sorted_model.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = sorted_model.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)\n",
    "\n",
    "# with sorting snake1d parameters: (tensor(5.9051), tensor(9.7172), tensor(0.1575))\n",
    "# without sorting snake1d parameters: (tensor(5.7267), tensor(9.3324), tensor(0.1360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py\", line 22, in fallback_function\n    shape = x.shape\n    x = x.reshape(shape[0], shape[1], -1)\n    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n                                                    ~~~~~~~~~ <--- HERE\n    x = x.reshape(shape)\n    return x\nRuntimeError: The size of tensor a (64) must match the size of tensor b (45) at non-singleton dimension 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m channel_pruning_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m      2\u001b[0m pruned_model \u001b[38;5;241m=\u001b[39m channel_prune(sorted_model, channel_pruning_ratio)\n\u001b[1;32m----> 4\u001b[0m artifact \u001b[38;5;241m=\u001b[39m \u001b[43mpruned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m recons \u001b[38;5;241m=\u001b[39m pruned_model\u001b[38;5;241m.\u001b[39mdecompress(artifact, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m waveform_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\base.py:212\u001b[0m, in \u001b[0;36mCodecMixin.compress\u001b[1;34m(self, audio_path_or_signal, win_duration, verbose, normalize_db, n_quantizers)\u001b[0m\n\u001b[0;32m    210\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39maudio_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    211\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(audio_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m--> 212\u001b[0m _, c, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m codes\u001b[38;5;241m.\u001b[39mappend(c\u001b[38;5;241m.\u001b[39mto(original_device))\n\u001b[0;32m    214\u001b[0m chunk_length \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:243\u001b[0m, in \u001b[0;36mDAC.encode\u001b[1;34m(self, audio_data, n_quantizers)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     audio_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    212\u001b[0m     n_quantizers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m ):\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode given audio data and return quantized latent codes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m            Number of samples in input audio\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     z, codes, latents, commitment_loss, codebook_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(\n\u001b[0;32m    245\u001b[0m         z, n_quantizers\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, codes, latents, commitment_loss, codebook_loss\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:91\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:61\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:36\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     pad \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py:33\u001b[0m, in \u001b[0;36mSnake1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msnake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py\", line 22, in fallback_function\n    shape = x.shape\n    x = x.reshape(shape[0], shape[1], -1)\n    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n                                                    ~~~~~~~~~ <--- HERE\n    x = x.reshape(shape)\n    return x\nRuntimeError: The size of tensor a (64) must match the size of tensor b (45) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "channel_pruning_ratio = 0.3\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "\n",
    "artifact = pruned_model.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = pruned_model.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def uniform_sparsity_testing(model_path, audio_file_path, channel_pruning_ratios):\n",
    "    mel_losses = []\n",
    "    stft_losses = []\n",
    "    waveform_losses = []\n",
    "\n",
    "    waveform_loss = losses.L1Loss()\n",
    "    stft_loss = losses.MultiScaleSTFTLoss()\n",
    "    mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "    signal = AudioSignal(audio_file_path)\n",
    "\n",
    "    for channel_pruning_ratio in tqdm(channel_pruning_ratios):\n",
    "        model = dac.DAC.load(model_path)\n",
    "\n",
    "        remove_wn(model.encoder.block)\n",
    "        remove_wn(model.decoder.model)\n",
    "\n",
    "        sorted_model = apply_channel_sorting(model)\n",
    "        pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "        all_convs = []\n",
    "        find_conv_layers(pruned_model.encoder.block, all_convs)\n",
    "        find_conv_layers(pruned_model.decoder.model, all_convs)\n",
    "        for conv in all_convs:\n",
    "            print(conv, conv.weight.shape)\n",
    "        print(all_convs[0].weight)\n",
    "\n",
    "        artifact = pruned_model.compress(signal.clone(), win_duration=5.0, verbose=False)\n",
    "        recons = pruned_model.decompress(artifact, verbose=False)\n",
    "\n",
    "        x = signal.clone().resample(44100)\n",
    "        y = recons.clone().resample(44100)\n",
    "\n",
    "        mel_losses.append(mel_loss(x, y))\n",
    "        stft_losses.append(stft_loss(x, y))\n",
    "        waveform_losses.append(waveform_loss(x, y))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(channel_pruning_ratios, mel_losses, label='Mel Loss')\n",
    "    plt.plot(channel_pruning_ratios, stft_losses, label='STFT Loss')\n",
    "    plt.plot(channel_pruning_ratios, waveform_losses, label='Waveform Loss')\n",
    "\n",
    "    plt.xlabel('Channel Pruning Ratio')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.title('Losses for Different Channel Pruning Ratios')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_path = \"../samples/UrbanSound8K 7383-3-0-0.wav\"\n",
    "channel_pruning_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "uniform_sparsity_testing(model_path, audio_file_path, channel_pruning_ratios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
