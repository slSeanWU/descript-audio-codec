{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dac\n",
    "from dac.model.dac import EncoderBlock, DecoderBlock, ResidualUnit\n",
    "from dac.nn.layers import Snake1d\n",
    "from audiotools import AudioSignal\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audiotools\\ml\\layers\\base.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = torch.load(location, \"cpu\")\n",
      "c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "model_path = dac.utils.download(model_type=\"44khz\")\n",
    "model = dac.DAC.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAC(\n",
       "  (encoder): Encoder(\n",
       "    (block): Sequential(\n",
       "      (0): Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): Snake1d()\n",
       "          (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "        )\n",
       "      )\n",
       "      (5): Snake1d()\n",
       "      (6): Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (quantizer): ResidualVectorQuantize(\n",
       "    (quantizers): ModuleList(\n",
       "      (0-8): 9 x VectorQuantize(\n",
       "        (in_proj): Conv1d(1024, 8, kernel_size=(1,), stride=(1,))\n",
       "        (out_proj): Conv1d(8, 1024, kernel_size=(1,), stride=(1,))\n",
       "        (codebook): Embedding(1024, 8)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (model): Sequential(\n",
       "      (0): Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (1): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Snake1d()\n",
       "          (1): ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "          (2): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (3): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "          (4): ResidualUnit(\n",
       "            (block): Sequential(\n",
       "              (0): Snake1d()\n",
       "              (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
       "              (2): Snake1d()\n",
       "              (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Snake1d()\n",
       "      (6): Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "nn.Conv1d here\n",
      "1 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "2 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,))\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "3 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "4 EncoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): Snake1d()\n",
      "    (4): Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "5 Snake1d()\n",
      "6 Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "nn.Conv1d here\n"
     ]
    }
   ],
   "source": [
    "conv_layers = []\n",
    "\n",
    "for idx, layer in enumerate(model.encoder.block):\n",
    "    print(idx, layer)\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        print(\"nn.Conv1d here\")\n",
    "        conv_layers.append(layer)\n",
    "    elif isinstance(layer, EncoderBlock):\n",
    "        encoder_block_layers = layer.block\n",
    "        print(\"encoder block here\")\n",
    "        for encoder_block_layer in encoder_block_layers:\n",
    "            if isinstance(encoder_block_layer, nn.Conv1d):\n",
    "                print(\"nn.Conv1d here\")\n",
    "                conv_layers.append(encoder_block_layer)\n",
    "            elif isinstance(encoder_block_layer, ResidualUnit):\n",
    "                residual_unit_layers = encoder_block_layer.block\n",
    "                print(\"residual unit here\")\n",
    "                for residual_unit_layer in residual_unit_layers:\n",
    "                    if isinstance(residual_unit_layer, nn.Conv1d):\n",
    "                        print(\"nn.Conv1d here\")\n",
    "                        conv_layers.append(residual_unit_layer)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "nn.Conv1d here\n",
      "1 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "2 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(384, 384, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "3 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "4 DecoderBlock(\n",
      "  (block): Sequential(\n",
      "    (0): Snake1d()\n",
      "    (1): ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (2): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualUnit(\n",
      "      (block): Sequential(\n",
      "        (0): Snake1d()\n",
      "        (1): Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,))\n",
      "        (2): Snake1d()\n",
      "        (3): Conv1d(96, 96, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder block here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "residual unit here\n",
      "nn.Conv1d here\n",
      "nn.Conv1d here\n",
      "5 Snake1d()\n",
      "6 Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "nn.Conv1d here\n",
      "7 Tanh()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       " Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       " Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       " Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       " Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       " Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       " Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       " Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       " Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       " Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       " Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       " Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       " Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       " Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       " Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers = []\n",
    "\n",
    "for idx, layer in enumerate(model.decoder.model):\n",
    "    print(idx, layer)\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        print(\"nn.Conv1d here\")\n",
    "        conv_layers.append(layer)\n",
    "    elif isinstance(layer, DecoderBlock):\n",
    "        decoder_block_layers = layer.block\n",
    "        print(\"encoder block here\")\n",
    "        for decoder_block_layer in decoder_block_layers:\n",
    "            if isinstance(decoder_block_layer, nn.Conv1d):\n",
    "                print(\"nn.Conv1d here\")\n",
    "                conv_layers.append(decoder_block_layer)\n",
    "            elif isinstance(decoder_block_layer, ResidualUnit):\n",
    "                residual_unit_layers = decoder_block_layer.block\n",
    "                print(\"residual unit here\")\n",
    "                for residual_unit_layer in residual_unit_layers:\n",
    "                    if isinstance(residual_unit_layer, nn.Conv1d):\n",
    "                        print(\"nn.Conv1d here\")\n",
    "                        conv_layers.append(residual_unit_layer)\n",
    "\n",
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(64, 64, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(64, 64, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(64, 64, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)),\n",
       "  Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(128, 128, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(128, 128, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(128, 128, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)),\n",
       "  Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(256, 256, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(256, 256, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(256, 256, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)),\n",
       "  Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(512, 512, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(512, 512, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(512, 512, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)),\n",
       "  Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,))],\n",
       " [Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)),\n",
       "  Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(768, 768, kernel_size=(1,), stride=(1,)),\n",
       "  ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)),\n",
       "  Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(384, 384, kernel_size=(1,), stride=(1,)),\n",
       "  ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)),\n",
       "  Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(192, 192, kernel_size=(1,), stride=(1,)),\n",
       "  ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)),\n",
       "  Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)),\n",
       "  Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)),\n",
       "  Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)),\n",
       "  Conv1d(96, 96, kernel_size=(1,), stride=(1,)),\n",
       "  Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,))])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_wn(layers):\n",
    "    for idx, layer in enumerate(layers):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            wn_removed_layer = nn.utils.remove_weight_norm(layer)\n",
    "            layers[idx] = wn_removed_layer\n",
    "        elif hasattr(layer, 'block'):\n",
    "            remove_wn(layer.block)\n",
    "\n",
    "def find_conv_layers(layers, conv_layers):\n",
    "    for idx, layer in enumerate(layers):\n",
    "        if isinstance(layer, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            conv_layers.append(layer)\n",
    "        elif hasattr(layer, 'block'):\n",
    "            find_conv_layers(layer.block, conv_layers)\n",
    "\n",
    "remove_wn(model.encoder.block)\n",
    "remove_wn(model.decoder.model)\n",
    "\n",
    "encoder_conv_layers = []\n",
    "decoder_conv_layers = []\n",
    "\n",
    "find_conv_layers(model.encoder.block, encoder_conv_layers)\n",
    "find_conv_layers(model.decoder.model, decoder_conv_layers)\n",
    "\n",
    "encoder_conv_layers, decoder_conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to sort the channels from important to non-important\n",
    "def get_input_channel_importance_conv1d(weight):\n",
    "    in_channels = weight.shape[1]\n",
    "    importances = []\n",
    "    # compute the importance for each input channel\n",
    "    for i_c in range(in_channels):\n",
    "        channel_weight = weight.detach()[:, i_c]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "# function to sort the channels from important to non-important\n",
    "def get_input_channel_importance_convtranspose1d(weight):\n",
    "    in_channels = weight.shape[0]\n",
    "    importances = []\n",
    "    # compute the importance for each input channel\n",
    "    for i_c in range(in_channels):\n",
    "        channel_weight = weight.detach()[i_c, :]\n",
    "        importance = torch.norm(channel_weight)\n",
    "        importances.append(importance.view(1))\n",
    "    return torch.cat(importances)\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_channel_sorting(model):\n",
    "    # model = copy.deepcopy(model)  # do not modify the original model\n",
    "    # fetch all the conv from the model\n",
    "    all_convs = []\n",
    "    find_conv_layers(model.encoder.block, all_convs)\n",
    "    find_conv_layers(model.decoder.model, all_convs)\n",
    "    # iterate through conv layers\n",
    "    for i_conv in range(len(all_convs) - 1):\n",
    "        # each channel sorting index, we need to apply it to:\n",
    "        # - the output dimension of the previous conv\n",
    "        # - the input dimension of the next conv (we compute importance here)\n",
    "        prev_conv = all_convs[i_conv]\n",
    "        next_conv = all_convs[i_conv + 1]\n",
    "        # note that we always compute the importance according to input channels\n",
    "        if (isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            importance = get_input_channel_importance_convtranspose1d(next_conv.weight)\n",
    "        else:\n",
    "            importance = get_input_channel_importance_conv1d(next_conv.weight)\n",
    "        # sorting from large to small\n",
    "        sort_idx = torch.argsort(importance, descending=True)\n",
    "        \n",
    "        # apply to the next conv input\n",
    "        if (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n",
    "        elif (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 0, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 0, sort_idx))\n",
    "        elif (isinstance(prev_conv, nn.ConvTranspose1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.copy_(torch.index_select(prev_conv.weight.detach(), 1, sort_idx))\n",
    "            next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))\n",
    "\n",
    "        prev_conv.bias.copy_(torch.index_select(prev_conv.bias.detach(), 0, sort_idx))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "def get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:\n",
    "    \"\"\"A function to calculate the number of layers to PRESERVE after pruning\n",
    "    Note that preserve_rate = 1. - prune_ratio\n",
    "    \"\"\"\n",
    "    return round((1 - prune_ratio) * channels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def channel_prune(model: nn.Module,\n",
    "                  prune_ratio: Union[List, float]) -> nn.Module:\n",
    "    \"\"\"Apply channel pruning to each of the conv layer in the backbone\n",
    "    Note that for prune_ratio, we can either provide a floating-point number,\n",
    "    indicating that we use a uniform pruning rate for all layers, or a list of\n",
    "    numbers to indicate per-layer pruning rate.\n",
    "    \"\"\"\n",
    "    # sanity check of provided prune_ratio\n",
    "    assert isinstance(prune_ratio, (float, list))\n",
    "    # fetch all the conv from the model\n",
    "    all_convs = []\n",
    "    find_conv_layers(model.encoder.block, all_convs)\n",
    "    find_conv_layers(model.decoder.model, all_convs)\n",
    "    n_conv = len(all_convs)\n",
    "    # note that for the ratios, it affects the previous conv output and next\n",
    "    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...\n",
    "    if isinstance(prune_ratio, list):\n",
    "        assert len(prune_ratio) == n_conv - 1\n",
    "    else:  # convert float to list\n",
    "        prune_ratio = [prune_ratio] * (n_conv - 1)\n",
    "\n",
    "    # we prune the convs in the backbone with a uniform ratio\n",
    "    # model = copy.deepcopy(model)  # prevent overwrite\n",
    "    # apply pruning. we naively keep the first k channels\n",
    "    for i_ratio, p_ratio in enumerate(prune_ratio):\n",
    "        prev_conv = all_convs[i_ratio]\n",
    "        next_conv = all_convs[i_ratio + 1]\n",
    "        original_channels = prev_conv.out_channels  # same as next_conv.in_channels\n",
    "        n_keep = get_num_channels_to_keep(original_channels, p_ratio)\n",
    "\n",
    "        if (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep, :])\n",
    "        elif (isinstance(prev_conv, nn.Conv1d) and isinstance(next_conv, nn.ConvTranspose1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:n_keep])\n",
    "        elif (isinstance(prev_conv, nn.ConvTranspose1d) and isinstance(next_conv, nn.Conv1d)):\n",
    "            prev_conv.weight.set_(prev_conv.weight.detach()[:, :n_keep, :])\n",
    "            next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep, :])\n",
    "\n",
    "        prev_conv.bias.set_(prev_conv.bias.detach()[:n_keep])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before pruning\n",
      "layer: Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([64, 1, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([64, 64, 7]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([64, 64, 1]) \tbias shape: torch.Size([64])\n",
      "layer: Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([128, 64, 4]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([128, 128, 7]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([128, 128, 1]) \tbias shape: torch.Size([128])\n",
      "layer: Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([256, 128, 8]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([256, 256, 7]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([256, 256, 1]) \tbias shape: torch.Size([256])\n",
      "layer: Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([512, 256, 16]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([512, 512, 7]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([512, 512, 1]) \tbias shape: torch.Size([512])\n",
      "layer: Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1024, 512, 16]) \tbias shape: torch.Size([1024])\n",
      "layer: Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) \tweight shape: torch.Size([1024, 1024, 3]) \tbias shape: torch.Size([1024])\n",
      "layer: Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1536, 1024, 7]) \tbias shape: torch.Size([1536])\n",
      "layer: ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1536, 768, 16]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([768, 768, 7]) \tbias shape: torch.Size([768])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([768, 768, 1]) \tbias shape: torch.Size([768])\n",
      "layer: ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([768, 384, 16]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([384, 384, 7]) \tbias shape: torch.Size([384])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([384, 384, 1]) \tbias shape: torch.Size([384])\n",
      "layer: ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([384, 192, 8]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([192, 192, 7]) \tbias shape: torch.Size([192])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([192, 192, 1]) \tbias shape: torch.Size([192])\n",
      "layer: ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([192, 96, 4]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([96, 96, 7]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([96, 96, 1]) \tbias shape: torch.Size([96])\n",
      "layer: Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1, 96, 7]) \tbias shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "model = dac.DAC.load(model_path)\n",
    "\n",
    "print(\"Before pruning\")\n",
    "all_convs = []\n",
    "find_conv_layers(model.encoder.block, all_convs)\n",
    "find_conv_layers(model.decoder.model, all_convs)\n",
    "for conv in all_convs:\n",
    "    print(\"layer:\", conv, \"\\tweight shape:\", conv.weight.shape, \"\\tbias shape:\", conv.bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_pruning_ratio = 0.3\n",
    "sorted_model = apply_channel_sorting(model)\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pruning\n",
      "layer: Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([45, 1, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([45, 45, 7]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 64, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([45, 45, 1]) \tbias shape: torch.Size([45])\n",
      "layer: Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([90, 45, 4]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([90, 90, 7]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 128, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([90, 90, 1]) \tbias shape: torch.Size([90])\n",
      "layer: Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([179, 90, 8]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([179, 179, 7]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 256, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([179, 179, 1]) \tbias shape: torch.Size([179])\n",
      "layer: Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([358, 179, 16]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([358, 358, 7]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 512, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([358, 358, 1]) \tbias shape: torch.Size([358])\n",
      "layer: Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([717, 358, 16]) \tbias shape: torch.Size([717])\n",
      "layer: Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) \tweight shape: torch.Size([717, 717, 3]) \tbias shape: torch.Size([717])\n",
      "layer: Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1075, 717, 7]) \tbias shape: torch.Size([1075])\n",
      "layer: ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([1075, 538, 16]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([538, 538, 7]) \tbias shape: torch.Size([538])\n",
      "layer: Conv1d(768, 768, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([538, 538, 1]) \tbias shape: torch.Size([538])\n",
      "layer: ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) \tweight shape: torch.Size([538, 269, 16]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([269, 269, 7]) \tbias shape: torch.Size([269])\n",
      "layer: Conv1d(384, 384, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([269, 269, 1]) \tbias shape: torch.Size([269])\n",
      "layer: ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) \tweight shape: torch.Size([269, 134, 8]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([134, 134, 7]) \tbias shape: torch.Size([134])\n",
      "layer: Conv1d(192, 192, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([134, 134, 1]) \tbias shape: torch.Size([134])\n",
      "layer: ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) \tweight shape: torch.Size([134, 67, 4]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) \tweight shape: torch.Size([67, 67, 7]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 96, kernel_size=(1,), stride=(1,)) \tweight shape: torch.Size([67, 67, 1]) \tbias shape: torch.Size([67])\n",
      "layer: Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) \tweight shape: torch.Size([1, 67, 7]) \tbias shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"After pruning\")\n",
    "all_convs = []\n",
    "find_conv_layers(pruned_model.encoder.block, all_convs)\n",
    "find_conv_layers(pruned_model.decoder.model, all_convs)\n",
    "for conv in all_convs:\n",
    "    print(\"layer:\", conv, \"\\tweight shape:\", conv.weight.shape, \"\\tbias shape:\", conv.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress and Decompress + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0102), tensor(2.0919), tensor(0.0179))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dac.utils import load_model\n",
    "from dac import DACFile\n",
    "from train import losses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "ref_generator = load_model(\n",
    "    model_type=\"44khz\",\n",
    "    model_bitrate=\"8kbps\",\n",
    "    tag=\"latest\",\n",
    "    load_path=model_path,\n",
    ")\n",
    "\n",
    "audio_file_path = \"../samples/UrbanSound8K 7383-3-0-0.wav\"\n",
    "\n",
    "signal = AudioSignal(audio_file_path)\n",
    "\n",
    "artifact = ref_generator.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = ref_generator.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DACFile(codes=tensor([[[568, 698, 481,  ..., 205, 283, 283],\n",
       "          [778, 245, 331,  ..., 678, 245, 538],\n",
       "          [708, 737, 182,  ...,  20, 624, 379],\n",
       "          ...,\n",
       "          [500, 263, 801,  ..., 120, 506, 687],\n",
       "          [860, 788, 943,  ..., 675, 997, 802],\n",
       "          [628, 367, 112,  ..., 366, 349, 597]],\n",
       " \n",
       "         [[568, 698, 618,  ..., 315, 283, 315],\n",
       "          [778, 245, 914,  ..., 217,   7, 898],\n",
       "          [566,  10, 879,  ..., 333, 624, 702],\n",
       "          ...,\n",
       "          [545,  63, 847,  ..., 972, 147, 668],\n",
       "          [614, 579, 746,  ..., 613, 153,  98],\n",
       "          [628, 827, 799,  ..., 929, 663, 870]]]), chunk_length=345, original_length=176400, input_db=tensor([-18.7919]), channels=2, sample_rate=44100, padding=True, dac_version='1.0.0'),\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235b561ef50>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact, recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<audiotools.core.audio_signal.AudioSignal at 0x235b559d3d0>,\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235df755610>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0102), tensor(2.0919), tensor(0.0179))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_wn(ref_generator.encoder.block)\n",
    "remove_wn(ref_generator.decoder.model)\n",
    "\n",
    "artifact = ref_generator.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = ref_generator.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DACFile(codes=tensor([[[568, 698, 481,  ..., 205, 283, 283],\n",
       "          [778, 245, 331,  ..., 678, 245, 538],\n",
       "          [708, 737, 182,  ...,  20, 624, 379],\n",
       "          ...,\n",
       "          [500, 263, 801,  ..., 120, 506, 687],\n",
       "          [860, 788, 943,  ..., 675, 997, 802],\n",
       "          [628, 367, 112,  ..., 366, 349, 597]],\n",
       " \n",
       "         [[568, 698, 618,  ..., 315, 283, 315],\n",
       "          [778, 245, 914,  ..., 217,   7, 898],\n",
       "          [566,  10, 879,  ..., 333, 624, 702],\n",
       "          ...,\n",
       "          [545,  63, 847,  ..., 972, 147, 668],\n",
       "          [614, 579, 746,  ..., 613, 153,  98],\n",
       "          [628, 827, 799,  ..., 929, 663, 870]]]), chunk_length=345, original_length=176400, input_db=tensor([-18.7919]), channels=2, sample_rate=44100, padding=True, dac_version='1.0.0'),\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235b55b68d0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact, recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<audiotools.core.audio_signal.AudioSignal at 0x235b5616d90>,\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235b561f510>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py\", line 22, in snake\n    shape = x.shape\n    x = x.reshape(shape[0], shape[1], -1)\n    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n                                                    ~~~~~~~~~ <--- HERE\n    x = x.reshape(shape)\n    return x\nRuntimeError: The size of tensor a (64) must match the size of tensor b (45) at non-singleton dimension 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[140], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m sorted_model \u001b[38;5;241m=\u001b[39m apply_channel_sorting(ref_generator)\n\u001b[0;32m      3\u001b[0m pruned_model \u001b[38;5;241m=\u001b[39m channel_prune(sorted_model, channel_pruning_ratio)\n\u001b[1;32m----> 5\u001b[0m artifact \u001b[38;5;241m=\u001b[39m \u001b[43mpruned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m recons \u001b[38;5;241m=\u001b[39m pruned_model\u001b[38;5;241m.\u001b[39mdecompress(artifact, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m waveform_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\base.py:212\u001b[0m, in \u001b[0;36mCodecMixin.compress\u001b[1;34m(self, audio_path_or_signal, win_duration, verbose, normalize_db, n_quantizers)\u001b[0m\n\u001b[0;32m    210\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39maudio_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    211\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(audio_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m--> 212\u001b[0m _, c, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m codes\u001b[38;5;241m.\u001b[39mappend(c\u001b[38;5;241m.\u001b[39mto(original_device))\n\u001b[0;32m    214\u001b[0m chunk_length \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:243\u001b[0m, in \u001b[0;36mDAC.encode\u001b[1;34m(self, audio_data, n_quantizers)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     audio_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    212\u001b[0m     n_quantizers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m ):\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode given audio data and return quantized latent codes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m            Number of samples in input audio\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     z, codes, latents, commitment_loss, codebook_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(\n\u001b[0;32m    245\u001b[0m         z, n_quantizers\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, codes, latents, commitment_loss, codebook_loss\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:91\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:61\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:36\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     pad \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py:33\u001b[0m, in \u001b[0;36mSnake1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msnake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\nn\\layers.py\", line 22, in snake\n    shape = x.shape\n    x = x.reshape(shape[0], shape[1], -1)\n    x = x + (alpha + 1e-9).reciprocal() * torch.sin(alpha * x).pow(2)\n                                                    ~~~~~~~~~ <--- HERE\n    x = x.reshape(shape)\n    return x\nRuntimeError: The size of tensor a (64) must match the size of tensor b (45) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "channel_pruning_ratio = 0.3\n",
    "sorted_model = apply_channel_sorting(ref_generator)\n",
    "pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "\n",
    "artifact = pruned_model.compress(signal, win_duration=5.0, verbose=False)\n",
    "\n",
    "recons = pruned_model.decompress(artifact, verbose=False)\n",
    "\n",
    "waveform_loss = losses.L1Loss()\n",
    "stft_loss = losses.MultiScaleSTFTLoss()\n",
    "mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "x = signal.clone().resample(44100)\n",
    "y = recons.clone().resample(44100)\n",
    "mel_loss(x, y), stft_loss(x, y), waveform_loss(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DACFile(codes=tensor([[[568, 698, 481,  ..., 205, 283, 283],\n",
       "          [778, 245, 331,  ..., 678, 245, 538],\n",
       "          [708, 737, 182,  ...,  20, 624, 379],\n",
       "          ...,\n",
       "          [500, 263, 801,  ..., 120, 506, 687],\n",
       "          [860, 788, 943,  ..., 675, 997, 802],\n",
       "          [628, 367, 112,  ..., 366, 349, 597]],\n",
       " \n",
       "         [[568, 698, 618,  ..., 315, 283, 315],\n",
       "          [778, 245, 914,  ..., 217,   7, 898],\n",
       "          [566,  10, 879,  ..., 333, 624, 702],\n",
       "          ...,\n",
       "          [545,  63, 847,  ..., 972, 147, 668],\n",
       "          [614, 579, 746,  ..., 613, 153,  98],\n",
       "          [628, 827, 799,  ..., 929, 663, 870]]]), chunk_length=345, original_length=176400, input_db=tensor([-18.7919]), channels=2, sample_rate=44100, padding=True, dac_version='1.0.0'),\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235b4c19cd0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact, recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<audiotools.core.audio_signal.AudioSignal at 0x235b53dd110>,\n",
       " <audiotools.core.audio_signal.AudioSignal at 0x235e4a300d0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def uniform_sparsity_testing(model_path, audio_file_path, channel_pruning_ratios):\n",
    "    mel_losses = []\n",
    "    stft_losses = []\n",
    "    waveform_losses = []\n",
    "\n",
    "    waveform_loss = losses.L1Loss()\n",
    "    stft_loss = losses.MultiScaleSTFTLoss()\n",
    "    mel_loss = losses.MelSpectrogramLoss()\n",
    "\n",
    "    signal = AudioSignal(audio_file_path)\n",
    "\n",
    "    for channel_pruning_ratio in tqdm(channel_pruning_ratios):\n",
    "        model = dac.DAC.load(model_path)\n",
    "\n",
    "        remove_wn(model.encoder.block)\n",
    "        remove_wn(model.decoder.model)\n",
    "\n",
    "        sorted_model = apply_channel_sorting(model)\n",
    "        pruned_model = channel_prune(sorted_model, channel_pruning_ratio)\n",
    "        all_convs = []\n",
    "        find_conv_layers(pruned_model.encoder.block, all_convs)\n",
    "        find_conv_layers(pruned_model.decoder.model, all_convs)\n",
    "        for conv in all_convs:\n",
    "            print(conv, conv.weight.shape)\n",
    "        print(all_convs[0].weight)\n",
    "\n",
    "        artifact = pruned_model.compress(signal.clone(), win_duration=5.0, verbose=False)\n",
    "        recons = pruned_model.decompress(artifact, verbose=False)\n",
    "\n",
    "        x = signal.clone().resample(44100)\n",
    "        y = recons.clone().resample(44100)\n",
    "\n",
    "        mel_losses.append(mel_loss(x, y))\n",
    "        stft_losses.append(stft_loss(x, y))\n",
    "        waveform_losses.append(waveform_loss(x, y))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(channel_pruning_ratios, mel_losses, label='Mel Loss')\n",
    "    plt.plot(channel_pruning_ratios, stft_losses, label='STFT Loss')\n",
    "    plt.plot(channel_pruning_ratios, waveform_losses, label='Waveform Loss')\n",
    "\n",
    "    plt.xlabel('Channel Pruning Ratio')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.title('Losses for Different Channel Pruning Ratios')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\audiotools\\ml\\layers\\base.py:172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_dict = torch.load(location, \"cpu\")\n",
      "c:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([64, 1, 7])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([64, 64, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([64, 64, 1])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([64, 64, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([64, 64, 1])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([64, 64, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([64, 64, 1])\n",
      "Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) torch.Size([128, 64, 4])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([128, 128, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([128, 128, 1])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([128, 128, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([128, 128, 1])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([128, 128, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([128, 128, 1])\n",
      "Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) torch.Size([256, 128, 8])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([256, 256, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([256, 256, 1])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([256, 256, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([256, 256, 1])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([256, 256, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([256, 256, 1])\n",
      "Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([512, 256, 16])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([512, 512, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([512, 512, 1])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([512, 512, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([512, 512, 1])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([512, 512, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([512, 512, 1])\n",
      "Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([1024, 512, 16])\n",
      "Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) torch.Size([1024, 1024, 3])\n",
      "Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([1536, 1024, 7])\n",
      "ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([1536, 768, 16])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([768, 768, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([768, 768, 1])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([768, 768, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([768, 768, 1])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([768, 768, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([768, 768, 1])\n",
      "ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([768, 384, 16])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([384, 384, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([384, 384, 1])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([384, 384, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([384, 384, 1])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([384, 384, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([384, 384, 1])\n",
      "ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) torch.Size([384, 192, 8])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([192, 192, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([192, 192, 1])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([192, 192, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([192, 192, 1])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([192, 192, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([192, 192, 1])\n",
      "ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) torch.Size([192, 96, 4])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([96, 96, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([96, 96, 1])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([96, 96, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([96, 96, 1])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([96, 96, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([96, 96, 1])\n",
      "Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([1, 96, 7])\n",
      "tensor([[[-0.0150,  0.0239,  0.0095,  0.0026,  0.0049, -0.0164,  0.0014]],\n",
      "\n",
      "        [[-0.0197, -0.0052,  0.0159, -0.0057,  0.0086, -0.0194,  0.0005]],\n",
      "\n",
      "        [[ 0.0406,  0.0259, -0.0102, -0.0121, -0.0050,  0.0342,  0.0106]],\n",
      "\n",
      "        [[ 0.0074,  0.0027, -0.0123, -0.0205,  0.0093,  0.0105,  0.0183]],\n",
      "\n",
      "        [[ 0.0512, -0.0130, -0.0117, -0.0111, -0.0264, -0.0528,  0.0151]],\n",
      "\n",
      "        [[ 0.0128, -0.0073,  0.0013, -0.0022, -0.0005, -0.0286,  0.0122]],\n",
      "\n",
      "        [[ 0.0096,  0.0067,  0.0116, -0.0102,  0.0033, -0.0014, -0.0006]],\n",
      "\n",
      "        [[-0.0288,  0.0066, -0.0057,  0.0139, -0.0174, -0.0002,  0.0025]],\n",
      "\n",
      "        [[-0.0367, -0.0267, -0.0216,  0.0057, -0.0083,  0.0322,  0.0071]],\n",
      "\n",
      "        [[-0.0157,  0.0195, -0.0098, -0.0183,  0.0100,  0.0248,  0.0198]],\n",
      "\n",
      "        [[-0.0180,  0.0338, -0.0186,  0.0264,  0.0214,  0.0236,  0.0055]],\n",
      "\n",
      "        [[ 0.0028,  0.0026,  0.0198, -0.0450, -0.0133, -0.0174,  0.0126]],\n",
      "\n",
      "        [[-0.0210,  0.0032, -0.0046, -0.0162, -0.0020, -0.0273,  0.0085]],\n",
      "\n",
      "        [[-0.0444, -0.0334, -0.0005,  0.0192, -0.0080, -0.0278,  0.0186]],\n",
      "\n",
      "        [[ 0.0159, -0.0046, -0.0233,  0.0065, -0.0142,  0.0135, -0.0047]],\n",
      "\n",
      "        [[-0.0172, -0.0059, -0.0081, -0.0133,  0.0166, -0.0144, -0.0105]],\n",
      "\n",
      "        [[-0.0338,  0.0223, -0.0096, -0.0123, -0.0137,  0.0069,  0.0097]],\n",
      "\n",
      "        [[-0.0185,  0.0446, -0.0382, -0.0309, -0.0249, -0.0100,  0.0283]],\n",
      "\n",
      "        [[ 0.0608, -0.0282,  0.0048,  0.0017, -0.0292, -0.0320,  0.0012]],\n",
      "\n",
      "        [[-0.0077, -0.0008,  0.0012, -0.0016, -0.0271, -0.0345,  0.0057]],\n",
      "\n",
      "        [[-0.0078,  0.0066,  0.0040,  0.0136, -0.0195, -0.0442,  0.0131]],\n",
      "\n",
      "        [[-0.0111, -0.0225,  0.0189,  0.0024, -0.0130, -0.0209, -0.0250]],\n",
      "\n",
      "        [[-0.0044, -0.0099,  0.0009,  0.0325, -0.0099,  0.0087,  0.0044]],\n",
      "\n",
      "        [[-0.0048, -0.0004,  0.0145,  0.0016, -0.0040, -0.0181, -0.0291]],\n",
      "\n",
      "        [[-0.0105,  0.0132, -0.0215, -0.0171, -0.0063,  0.0088, -0.0067]],\n",
      "\n",
      "        [[-0.0095,  0.0332, -0.0341,  0.0243,  0.0029,  0.0060, -0.0304]],\n",
      "\n",
      "        [[ 0.0106, -0.0239,  0.0153, -0.0287,  0.0016, -0.0074, -0.0056]],\n",
      "\n",
      "        [[-0.0168,  0.0144, -0.0325, -0.0137, -0.0109, -0.0012, -0.0187]],\n",
      "\n",
      "        [[ 0.0090,  0.0100,  0.0174,  0.0247, -0.0204, -0.0079, -0.0154]],\n",
      "\n",
      "        [[ 0.0142, -0.0175,  0.0085, -0.0150,  0.0119,  0.0114,  0.0430]],\n",
      "\n",
      "        [[-0.0058, -0.0136,  0.0021,  0.0134, -0.0233, -0.0233,  0.0329]],\n",
      "\n",
      "        [[ 0.0173,  0.0090,  0.0089,  0.0132, -0.0356, -0.0009,  0.0104]],\n",
      "\n",
      "        [[-0.0109, -0.0088, -0.0011,  0.0214, -0.0408, -0.0163, -0.0028]],\n",
      "\n",
      "        [[ 0.0352,  0.0186,  0.0241,  0.0002,  0.0136, -0.0052, -0.0078]],\n",
      "\n",
      "        [[ 0.0164, -0.0009, -0.0165, -0.0248,  0.0050,  0.0164,  0.0063]],\n",
      "\n",
      "        [[-0.0068, -0.0012,  0.0112,  0.0317, -0.0281, -0.0108, -0.0019]],\n",
      "\n",
      "        [[-0.0313,  0.0041,  0.0242,  0.0232,  0.0204, -0.0266,  0.0413]],\n",
      "\n",
      "        [[ 0.0213,  0.0127,  0.0078, -0.0209,  0.0045, -0.0094, -0.0199]],\n",
      "\n",
      "        [[-0.0253,  0.0166,  0.0205,  0.0043, -0.0359, -0.0377,  0.0275]],\n",
      "\n",
      "        [[ 0.0203,  0.0082, -0.0152, -0.0210,  0.0112,  0.0009, -0.0102]],\n",
      "\n",
      "        [[-0.0062, -0.0096,  0.0179, -0.0297,  0.0148, -0.0066,  0.0168]],\n",
      "\n",
      "        [[ 0.0396, -0.0112,  0.0062, -0.0129,  0.0178,  0.0155, -0.0110]],\n",
      "\n",
      "        [[-0.0051, -0.0191, -0.0069,  0.0157,  0.0089, -0.0409, -0.0461]],\n",
      "\n",
      "        [[ 0.0044,  0.0035,  0.0235,  0.0004,  0.0431, -0.0371, -0.0095]],\n",
      "\n",
      "        [[-0.0104, -0.0104, -0.0432,  0.0147,  0.0283,  0.0264, -0.0093]],\n",
      "\n",
      "        [[-0.0037, -0.0022,  0.0315, -0.0147,  0.0187, -0.0379, -0.0057]],\n",
      "\n",
      "        [[ 0.0114,  0.0174, -0.0023, -0.0091, -0.0162, -0.0260,  0.0133]],\n",
      "\n",
      "        [[ 0.0374,  0.0095,  0.0052,  0.0033,  0.0192,  0.0018,  0.0006]],\n",
      "\n",
      "        [[-0.0063, -0.0078, -0.0150,  0.0139,  0.0051, -0.0157, -0.0139]],\n",
      "\n",
      "        [[ 0.0134, -0.0021,  0.0066,  0.0105,  0.0060,  0.0115,  0.0012]],\n",
      "\n",
      "        [[ 0.0035, -0.0345,  0.0076, -0.0141,  0.0063, -0.0051,  0.0371]],\n",
      "\n",
      "        [[ 0.0057,  0.0226, -0.0115, -0.0007, -0.0071,  0.0193, -0.0022]],\n",
      "\n",
      "        [[-0.0301, -0.0240, -0.0170, -0.0123,  0.0427,  0.0215, -0.0276]],\n",
      "\n",
      "        [[-0.0312,  0.0159, -0.0001,  0.0287,  0.0300, -0.0127,  0.0062]],\n",
      "\n",
      "        [[-0.0161, -0.0039,  0.0277, -0.0262, -0.0214, -0.0021,  0.0474]],\n",
      "\n",
      "        [[-0.0062, -0.0138,  0.0018, -0.0263,  0.0253,  0.0326, -0.0013]],\n",
      "\n",
      "        [[ 0.0001, -0.0160, -0.0231, -0.0151, -0.0066, -0.0272, -0.0134]],\n",
      "\n",
      "        [[-0.0199, -0.0239, -0.0238, -0.0300, -0.0071, -0.0042,  0.0075]],\n",
      "\n",
      "        [[-0.0086, -0.0126,  0.0478, -0.0586,  0.0089, -0.0016, -0.0043]],\n",
      "\n",
      "        [[-0.0153, -0.0083, -0.0441,  0.0002,  0.0105,  0.0084,  0.0078]],\n",
      "\n",
      "        [[ 0.0058, -0.0031, -0.0001,  0.0047,  0.0243, -0.0365, -0.0161]],\n",
      "\n",
      "        [[ 0.0112, -0.0191, -0.0065, -0.0127,  0.0043, -0.0314,  0.0130]],\n",
      "\n",
      "        [[ 0.0102, -0.0055, -0.0326,  0.0169, -0.0053, -0.0183, -0.0034]],\n",
      "\n",
      "        [[-0.0273, -0.0192, -0.0101,  0.0022, -0.0206, -0.0282,  0.0025]]],\n",
      "       grad_fn=<WeightNormInterfaceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:08<00:57,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1d(1, 64, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([58, 1, 7])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([58, 58, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([58, 58, 1])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([58, 58, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([58, 58, 1])\n",
      "Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([58, 58, 7])\n",
      "Conv1d(64, 64, kernel_size=(1,), stride=(1,)) torch.Size([58, 58, 1])\n",
      "Conv1d(64, 128, kernel_size=(4,), stride=(2,), padding=(1,)) torch.Size([115, 58, 4])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([115, 115, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([115, 115, 1])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([115, 115, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([115, 115, 1])\n",
      "Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([115, 115, 7])\n",
      "Conv1d(128, 128, kernel_size=(1,), stride=(1,)) torch.Size([115, 115, 1])\n",
      "Conv1d(128, 256, kernel_size=(8,), stride=(4,), padding=(2,)) torch.Size([230, 115, 8])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([230, 230, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([230, 230, 1])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([230, 230, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([230, 230, 1])\n",
      "Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([230, 230, 7])\n",
      "Conv1d(256, 256, kernel_size=(1,), stride=(1,)) torch.Size([230, 230, 1])\n",
      "Conv1d(256, 512, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([461, 230, 16])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([461, 461, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([461, 461, 1])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([461, 461, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([461, 461, 1])\n",
      "Conv1d(512, 512, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([461, 461, 7])\n",
      "Conv1d(512, 512, kernel_size=(1,), stride=(1,)) torch.Size([461, 461, 1])\n",
      "Conv1d(512, 1024, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([922, 461, 16])\n",
      "Conv1d(1024, 1024, kernel_size=(3,), stride=(1,), padding=(1,)) torch.Size([922, 922, 3])\n",
      "Conv1d(1024, 1536, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([1382, 922, 7])\n",
      "ConvTranspose1d(1536, 768, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([1382, 691, 16])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([691, 691, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([691, 691, 1])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([691, 691, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([691, 691, 1])\n",
      "Conv1d(768, 768, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([691, 691, 7])\n",
      "Conv1d(768, 768, kernel_size=(1,), stride=(1,)) torch.Size([691, 691, 1])\n",
      "ConvTranspose1d(768, 384, kernel_size=(16,), stride=(8,), padding=(4,)) torch.Size([691, 346, 16])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([346, 346, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([346, 346, 1])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([346, 346, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([346, 346, 1])\n",
      "Conv1d(384, 384, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([346, 346, 7])\n",
      "Conv1d(384, 384, kernel_size=(1,), stride=(1,)) torch.Size([346, 346, 1])\n",
      "ConvTranspose1d(384, 192, kernel_size=(8,), stride=(4,), padding=(2,)) torch.Size([346, 173, 8])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([173, 173, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([173, 173, 1])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([173, 173, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([173, 173, 1])\n",
      "Conv1d(192, 192, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([173, 173, 7])\n",
      "Conv1d(192, 192, kernel_size=(1,), stride=(1,)) torch.Size([173, 173, 1])\n",
      "ConvTranspose1d(192, 96, kernel_size=(4,), stride=(2,), padding=(1,)) torch.Size([173, 86, 4])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([86, 86, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([86, 86, 1])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,)) torch.Size([86, 86, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([86, 86, 1])\n",
      "Conv1d(96, 96, kernel_size=(7,), stride=(1,), padding=(27,), dilation=(9,)) torch.Size([86, 86, 7])\n",
      "Conv1d(96, 96, kernel_size=(1,), stride=(1,)) torch.Size([86, 86, 1])\n",
      "Conv1d(96, 1, kernel_size=(7,), stride=(1,), padding=(3,)) torch.Size([1, 86, 7])\n",
      "tensor([[[-3.7652e-03, -4.3011e-02, -1.6531e-02, -2.3931e-02,  6.6207e-03,\n",
      "          -5.4272e-03, -1.3539e-02]],\n",
      "\n",
      "        [[ 4.8728e-03, -6.2725e-03, -2.7224e-02,  2.4723e-02, -2.7542e-02,\n",
      "           2.4484e-02, -3.1344e-02]],\n",
      "\n",
      "        [[-3.0722e-03,  7.6298e-03, -2.3573e-02, -6.2949e-03,  3.4046e-03,\n",
      "           3.1343e-02,  1.1646e-02]],\n",
      "\n",
      "        [[ 7.7589e-03, -2.5412e-02, -8.6880e-03,  3.5868e-02, -6.8607e-03,\n",
      "           1.3728e-02,  1.6895e-02]],\n",
      "\n",
      "        [[-2.0635e-02,  1.0867e-02,  7.7681e-03,  1.0927e-02,  1.4491e-02,\n",
      "          -1.9233e-02,  2.9363e-04]],\n",
      "\n",
      "        [[ 7.5653e-03, -3.1274e-02, -3.1031e-03,  4.3766e-03, -2.9729e-02,\n",
      "          -2.2111e-02, -4.0060e-03]],\n",
      "\n",
      "        [[-3.7111e-04,  1.1154e-02, -2.4587e-02, -6.7853e-03, -1.2084e-02,\n",
      "          -7.5488e-03,  1.6197e-02]],\n",
      "\n",
      "        [[-6.7558e-03,  2.8433e-02, -2.2765e-02, -1.0339e-02, -1.3874e-03,\n",
      "          -1.1585e-02, -2.3674e-02]],\n",
      "\n",
      "        [[-1.2997e-03, -2.4298e-02, -2.5669e-02,  4.9102e-03, -2.7702e-02,\n",
      "          -4.1102e-02,  1.6849e-02]],\n",
      "\n",
      "        [[ 1.3696e-02,  5.3333e-02,  2.8139e-02,  3.8417e-02, -7.7481e-04,\n",
      "          -4.7371e-03,  2.8290e-02]],\n",
      "\n",
      "        [[-8.7509e-03,  2.8106e-02, -9.5280e-03,  1.4834e-02, -8.1184e-03,\n",
      "          -3.7854e-02,  1.6381e-03]],\n",
      "\n",
      "        [[ 2.1023e-02, -2.0768e-02, -2.4612e-02, -4.9068e-02, -6.4946e-03,\n",
      "           1.8216e-02,  1.3829e-02]],\n",
      "\n",
      "        [[-9.2168e-03,  1.0519e-02, -9.4606e-03,  1.0892e-02, -8.7941e-06,\n",
      "          -1.1528e-02,  3.1284e-02]],\n",
      "\n",
      "        [[ 8.1682e-03,  2.1417e-02, -1.0683e-02, -4.2764e-02,  4.3807e-02,\n",
      "          -5.9899e-03, -8.4698e-04]],\n",
      "\n",
      "        [[-6.2311e-03,  1.1108e-02, -3.1453e-02, -1.1795e-02,  1.6360e-02,\n",
      "          -1.0043e-02,  1.5853e-02]],\n",
      "\n",
      "        [[ 8.9061e-03, -1.6335e-02, -1.0647e-02, -9.1978e-03,  1.2057e-02,\n",
      "          -1.8067e-02,  7.2384e-03]],\n",
      "\n",
      "        [[-2.3827e-02, -8.7945e-03, -1.9557e-02,  1.3173e-02, -2.6606e-02,\n",
      "           1.6544e-02, -1.8929e-02]],\n",
      "\n",
      "        [[-1.8742e-02, -5.6780e-04,  1.8099e-02,  1.3728e-03, -2.0316e-02,\n",
      "          -7.9599e-03,  1.7489e-02]],\n",
      "\n",
      "        [[ 1.5799e-04,  8.5158e-03,  1.6450e-02, -3.4909e-02, -2.5509e-02,\n",
      "          -9.6398e-03,  1.7456e-02]],\n",
      "\n",
      "        [[ 1.2454e-02,  3.7918e-02, -2.4686e-02,  1.2616e-02,  6.1841e-03,\n",
      "           1.4276e-02,  1.1816e-02]],\n",
      "\n",
      "        [[ 3.0660e-02, -9.0886e-03,  6.4406e-03, -6.6962e-02, -1.5075e-02,\n",
      "           1.3140e-03,  3.5105e-02]],\n",
      "\n",
      "        [[ 1.3008e-02, -9.2854e-03, -2.1900e-02,  4.6401e-03, -1.6887e-02,\n",
      "          -1.0551e-02,  1.1420e-02]],\n",
      "\n",
      "        [[ 1.7993e-02, -1.2958e-02, -1.0952e-02,  9.3337e-03, -2.8982e-02,\n",
      "           7.5418e-03,  5.6376e-03]],\n",
      "\n",
      "        [[ 5.3399e-03,  9.6378e-03,  3.6371e-02,  1.3886e-02,  2.2631e-02,\n",
      "           2.4388e-02, -2.1751e-02]],\n",
      "\n",
      "        [[ 2.7262e-02,  7.2695e-03,  2.6268e-02, -4.5688e-02, -3.5174e-03,\n",
      "          -1.1350e-02,  3.8422e-03]],\n",
      "\n",
      "        [[ 2.9635e-03, -2.7114e-02,  2.1239e-02,  2.1933e-02,  1.8565e-02,\n",
      "           1.4534e-02, -2.0861e-02]],\n",
      "\n",
      "        [[ 2.4622e-05, -1.9402e-03,  1.2125e-02,  2.7786e-02, -1.3956e-02,\n",
      "          -1.4482e-03,  3.5423e-02]],\n",
      "\n",
      "        [[ 3.3163e-03, -8.5031e-03,  2.7657e-02, -1.6429e-02,  1.1964e-03,\n",
      "           1.6201e-02, -3.4511e-03]],\n",
      "\n",
      "        [[ 1.7634e-02,  2.7649e-03,  6.9876e-03,  2.3620e-02, -2.6794e-03,\n",
      "          -1.9219e-02, -3.8448e-02]],\n",
      "\n",
      "        [[-4.1659e-02, -8.6328e-03,  2.4827e-02, -1.4732e-02,  1.3646e-02,\n",
      "          -1.3301e-02, -3.2237e-02]],\n",
      "\n",
      "        [[ 2.2690e-02, -6.9747e-03, -1.4157e-02,  1.5906e-02,  3.4847e-03,\n",
      "           1.2298e-02, -1.0456e-02]],\n",
      "\n",
      "        [[ 1.8343e-02, -1.9383e-02,  2.4113e-02,  7.0764e-03, -5.0148e-03,\n",
      "          -3.0784e-02,  1.9437e-02]],\n",
      "\n",
      "        [[-2.0191e-02, -1.3999e-03, -6.5652e-03,  1.7412e-02, -5.7487e-03,\n",
      "          -1.1282e-02,  2.3848e-02]],\n",
      "\n",
      "        [[-8.7788e-04, -1.1578e-03,  1.2553e-02,  3.0528e-02, -4.3774e-03,\n",
      "          -1.7177e-03, -1.2022e-02]],\n",
      "\n",
      "        [[ 2.2770e-02, -1.2229e-02,  9.3907e-03,  2.8678e-02, -1.5102e-02,\n",
      "           4.9165e-03,  1.9041e-02]],\n",
      "\n",
      "        [[ 3.4692e-03, -1.9297e-02,  2.7707e-02,  1.9094e-02, -3.7927e-02,\n",
      "          -2.8001e-02, -1.8267e-02]],\n",
      "\n",
      "        [[ 1.1040e-02, -5.3383e-03, -2.1463e-02,  6.3589e-03,  3.5767e-02,\n",
      "           3.9286e-03,  6.6276e-02]],\n",
      "\n",
      "        [[-2.8798e-02, -2.5301e-02,  1.0916e-02, -3.4243e-02, -2.8903e-02,\n",
      "           2.6592e-02, -2.5184e-02]],\n",
      "\n",
      "        [[ 2.3811e-02,  2.9738e-02,  9.7567e-03, -2.3241e-03,  2.5529e-02,\n",
      "           1.6406e-02, -2.7438e-02]],\n",
      "\n",
      "        [[-2.6845e-02, -6.2325e-02, -1.9220e-02,  3.0857e-03,  1.0157e-02,\n",
      "           5.1623e-02,  2.2313e-04]],\n",
      "\n",
      "        [[ 2.5729e-02, -1.8155e-02, -1.9804e-02, -4.2787e-02, -2.0785e-02,\n",
      "          -5.7637e-03, -2.1213e-02]],\n",
      "\n",
      "        [[-3.1916e-03,  2.6675e-03,  3.2591e-03, -1.5091e-02, -2.4918e-04,\n",
      "          -9.5509e-03, -2.3938e-02]],\n",
      "\n",
      "        [[ 2.9123e-03,  2.7009e-02, -2.7601e-02, -1.6657e-02, -1.9401e-02,\n",
      "           1.4772e-02, -1.1149e-02]],\n",
      "\n",
      "        [[ 2.8997e-03, -1.4698e-02, -1.2304e-02, -1.4995e-02,  1.3783e-02,\n",
      "          -1.3096e-02, -2.2443e-02]],\n",
      "\n",
      "        [[-3.3254e-02,  4.6762e-02, -1.9783e-02,  6.7225e-03, -2.1097e-02,\n",
      "          -2.0464e-02,  7.8994e-03]],\n",
      "\n",
      "        [[-1.1774e-03,  2.5065e-03, -1.3911e-02,  9.8101e-03,  1.1044e-02,\n",
      "          -2.9189e-02, -1.0258e-02]],\n",
      "\n",
      "        [[ 1.4322e-03, -3.3663e-03,  1.4201e-02,  6.9274e-05, -7.5771e-03,\n",
      "           1.7470e-02, -2.8293e-02]],\n",
      "\n",
      "        [[-8.5278e-03,  1.2656e-02, -1.3285e-02, -9.8790e-03,  1.0753e-02,\n",
      "           1.4127e-02,  1.1722e-03]],\n",
      "\n",
      "        [[ 4.6770e-04, -1.7244e-03,  6.7544e-02, -2.0659e-02,  1.5616e-02,\n",
      "           3.0672e-03,  7.4028e-03]],\n",
      "\n",
      "        [[ 6.1729e-05,  3.7213e-03,  1.1498e-02, -1.2796e-03, -3.4500e-02,\n",
      "          -2.9596e-02,  7.8303e-03]],\n",
      "\n",
      "        [[-1.2914e-02, -3.1808e-03,  5.4986e-03,  1.8335e-03,  6.3628e-03,\n",
      "           2.1019e-03,  1.9099e-02]],\n",
      "\n",
      "        [[-1.1359e-02, -1.0323e-02, -9.8233e-03,  4.8886e-04, -2.8134e-02,\n",
      "          -1.1426e-03, -3.1076e-03]],\n",
      "\n",
      "        [[ 6.0994e-02,  2.7860e-02,  2.9596e-02, -1.1368e-02,  4.5988e-02,\n",
      "          -8.3013e-03,  7.2997e-03]],\n",
      "\n",
      "        [[ 1.5501e-02,  5.8150e-03,  6.5348e-03,  2.9747e-02, -9.6063e-03,\n",
      "           1.4109e-02, -4.0586e-03]],\n",
      "\n",
      "        [[ 1.2890e-02, -2.2937e-02,  3.3370e-02,  6.9901e-03,  2.0223e-02,\n",
      "          -2.8191e-02,  4.2884e-03]],\n",
      "\n",
      "        [[-2.0335e-02, -8.3351e-03, -2.5979e-03,  2.8299e-02,  1.6997e-04,\n",
      "           1.9089e-02,  2.0583e-02]],\n",
      "\n",
      "        [[-3.5420e-02, -2.0509e-02, -2.0074e-02, -3.2006e-02,  2.0810e-02,\n",
      "           2.2339e-02, -3.0366e-02]],\n",
      "\n",
      "        [[-1.2843e-02,  1.6801e-02, -1.5134e-02, -2.3452e-02,  1.2921e-02,\n",
      "           1.4387e-03, -2.8766e-02]]], grad_fn=<WeightNormInterfaceBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:11<01:21, 11.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m audio_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../samples/UrbanSound8K 7383-3-0-0.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m channel_pruning_ratios \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43muniform_sparsity_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_pruning_ratios\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[84], line 29\u001b[0m, in \u001b[0;36muniform_sparsity_testing\u001b[1;34m(model_path, audio_file_path, channel_pruning_ratios)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(conv, conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_convs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m---> 29\u001b[0m artifact \u001b[38;5;241m=\u001b[39m \u001b[43mpruned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwin_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m recons \u001b[38;5;241m=\u001b[39m pruned_model\u001b[38;5;241m.\u001b[39mdecompress(artifact, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mresample(\u001b[38;5;241m44100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\base.py:212\u001b[0m, in \u001b[0;36mCodecMixin.compress\u001b[1;34m(self, audio_path_or_signal, win_duration, verbose, normalize_db, n_quantizers)\u001b[0m\n\u001b[0;32m    210\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39maudio_data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    211\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(audio_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\n\u001b[1;32m--> 212\u001b[0m _, c, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quantizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m codes\u001b[38;5;241m.\u001b[39mappend(c\u001b[38;5;241m.\u001b[39mto(original_device))\n\u001b[0;32m    214\u001b[0m chunk_length \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:243\u001b[0m, in \u001b[0;36mDAC.encode\u001b[1;34m(self, audio_data, n_quantizers)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     audio_data: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    212\u001b[0m     n_quantizers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m ):\n\u001b[0;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode given audio data and return quantized latent codes\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m            Number of samples in input audio\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     z, codes, latents, commitment_loss, codebook_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantizer(\n\u001b[0;32m    245\u001b[0m         z, n_quantizers\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, codes, latents, commitment_loss, codebook_loss\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:91\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:61\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dac\\model\\dac.py:36\u001b[0m, in \u001b[0;36mResidualUnit.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 36\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     pad \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pad \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chenk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "audio_file_path = \"../samples/UrbanSound8K 7383-3-0-0.wav\"\n",
    "channel_pruning_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "uniform_sparsity_testing(model_path, audio_file_path, channel_pruning_ratios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
